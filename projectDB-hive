In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0016, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0016
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0016
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 00:33:29,641 Stage-1 map = 0%,  reduce = 0%
2023-07-20 00:33:31,651 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.58 sec
2023-07-20 00:33:32,656 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.58 sec
2023-07-20 00:33:33,662 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.58 sec
2023-07-20 00:33:34,667 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.13 sec
2023-07-20 00:33:35,673 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.13 sec
2023-07-20 00:33:36,685 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.13 sec
MapReduce Total cumulative CPU time: 2 seconds 130 msec
Ended Job = job_202307191745_0016
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.13 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 130 msec
OK
3
1
5
4
Time taken: 11.278 seconds
hive> select page,count(*) from clickstream group by page; 
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0017, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0017
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0017
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 00:33:58,604 Stage-1 map = 0%,  reduce = 0%
2023-07-20 00:34:00,612 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.65 sec
2023-07-20 00:34:01,618 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.65 sec
2023-07-20 00:34:02,624 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.65 sec
2023-07-20 00:34:03,630 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.92 sec
2023-07-20 00:34:04,637 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.92 sec
2023-07-20 00:34:05,644 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.92 sec
2023-07-20 00:34:06,659 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.92 sec
MapReduce Total cumulative CPU time: 1 seconds 920 msec
Ended Job = job_202307191745_0017
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 1.92 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 920 msec
OK
cart_page	3
checkout_page	1
homepage	5
product_page	4
Time taken: 11.806 seconds
hive> select ts, avg(amount) from purchase group by ts;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0018, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0018
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0018
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 02:11:51,780 Stage-1 map = 0%,  reduce = 0%
2023-07-20 02:11:53,795 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.63 sec
2023-07-20 02:11:54,804 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.63 sec
2023-07-20 02:11:55,810 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.63 sec
2023-07-20 02:11:56,815 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
2023-07-20 02:11:57,822 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
2023-07-20 02:11:58,834 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
MapReduce Total cumulative CPU time: 2 seconds 150 msec
Ended Job = job_202307191745_0018
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.15 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 150 msec
OK
2023-01-01 10:05:00	100.0
2023-01-01 10:08:00	150.0
2023-01-01 10:09:00	200.0
2023-01-01 10:13:00	120.0
2023-01-01 10:17:00	80.0
Time taken: 11.2 seconds
hive> select avg(amount) from purchase;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0019, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0019
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0019
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 02:12:42,572 Stage-1 map = 0%,  reduce = 0%
2023-07-20 02:12:45,610 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.63 sec
2023-07-20 02:12:46,616 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.63 sec
2023-07-20 02:12:47,621 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.63 sec
2023-07-20 02:12:48,626 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.3 sec
2023-07-20 02:12:49,631 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.3 sec
2023-07-20 02:12:50,641 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.3 sec
MapReduce Total cumulative CPU time: 2 seconds 300 msec
Ended Job = job_202307191745_0019
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.3 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 300 msec
OK
130.0
Time taken: 11.599 seconds
hive> select * from customer;
OK
1	John Doe	john.doe@example.com
2	Jane Smith	jane.smith@example.com
3	Robert Johnson	robert.johnson@example.com
4	Lisa Brown	lisa.brown@example.com
5	Michael Wilson	michael.wilson@example.com
Time taken: 0.136 seconds
hive> select email,count(email) from clickstream group by userId having count(email) > 1;    
FAILED: SemanticException [Error 10004]: Line 1:72 Invalid table alias or column reference 'email': (possible column names are: userid, ts, page)
hive> select email,count(email) from customer group by userId having count(email) > 1;    
FAILED: SemanticException [Error 10025]: Line 1:7 Expression not in GROUP BY key 'email'
hive> select email,count(email) from customer group by email having count(email) > 1;  
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0020, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0020
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0020
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 03:23:58,780 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:24:00,787 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-07-20 03:24:01,793 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-07-20 03:24:02,798 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-07-20 03:24:03,806 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
2023-07-20 03:24:04,811 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
2023-07-20 03:24:05,822 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
MapReduce Total cumulative CPU time: 2 seconds 150 msec
Ended Job = job_202307191745_0020
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.15 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 150 msec
OK
Time taken: 10.609 seconds
hive> select userId,count(email) from customer group by email having count(email) > 1; 
FAILED: SemanticException [Error 10025]: Line 1:7 Expression not in GROUP BY key 'userId'
hive> select userId,count(email) from customer group by userId having count(email) > 1; 
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0021, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0021
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0021
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 03:25:30,834 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:25:32,842 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.7 sec
2023-07-20 03:25:33,849 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.7 sec
2023-07-20 03:25:34,856 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.7 sec
2023-07-20 03:25:35,863 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.12 sec
2023-07-20 03:25:36,868 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.12 sec
2023-07-20 03:25:37,880 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.12 sec
MapReduce Total cumulative CPU time: 2 seconds 120 msec
Ended Job = job_202307191745_0021
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.12 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 120 msec
OK
Time taken: 10.632 seconds
hive> select email,count(*) from customer group by email;                               
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0022, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0022
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0022
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 03:27:28,710 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:27:30,720 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-07-20 03:27:31,725 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-07-20 03:27:32,735 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-07-20 03:27:33,739 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.1 sec
2023-07-20 03:27:34,745 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.1 sec
2023-07-20 03:27:35,778 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.1 sec
MapReduce Total cumulative CPU time: 2 seconds 100 msec
Ended Job = job_202307191745_0022
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.1 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 100 msec
OK
jane.smith@example.com	1
john.doe@example.com	1
lisa.brown@example.com	1
michael.wilson@example.com	1
robert.johnson@example.com	1
Time taken: 10.755 seconds
hive> select * from purchase order by amount;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0023, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0023
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0023
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 03:29:43,659 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:29:45,665 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.64 sec
2023-07-20 03:29:46,671 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.64 sec
2023-07-20 03:29:47,675 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.64 sec
2023-07-20 03:29:48,680 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
2023-07-20 03:29:49,690 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
2023-07-20 03:29:50,702 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
MapReduce Total cumulative CPU time: 2 seconds 150 msec
Ended Job = job_202307191745_0023
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.15 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 150 msec
OK
5	2023-01-01 10:17:00	80
1	2023-01-01 10:05:00	100
4	2023-01-01 10:13:00	120
2	2023-01-01 10:08:00	150
3	2023-01-01 10:09:00	200
Time taken: 10.618 seconds
hive> select * from purchase order by amount desc;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0024, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0024
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0024
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 03:30:39,579 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:30:41,586 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.57 sec
2023-07-20 03:30:42,593 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.57 sec
2023-07-20 03:30:43,598 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.57 sec
2023-07-20 03:30:44,603 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.09 sec
2023-07-20 03:30:45,609 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.09 sec
2023-07-20 03:30:46,621 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.09 sec
MapReduce Total cumulative CPU time: 2 seconds 90 msec
Ended Job = job_202307191745_0024
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.09 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 90 msec
OK
3	2023-01-01 10:09:00	200
2	2023-01-01 10:08:00	150
4	2023-01-01 10:13:00	120
1	2023-01-01 10:05:00	100
5	2023-01-01 10:17:00	80
Time taken: 10.532 seconds
hive> select * from purchase sort by amount desc;      
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0025, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0025
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0025
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 03:32:49,421 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:32:52,436 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.61 sec
2023-07-20 03:32:53,441 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.61 sec
2023-07-20 03:32:54,446 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.07 sec
2023-07-20 03:32:55,450 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.07 sec
2023-07-20 03:32:56,465 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.07 sec
MapReduce Total cumulative CPU time: 2 seconds 70 msec
Ended Job = job_202307191745_0025
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.07 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 70 msec
OK
3	2023-01-01 10:09:00	200
2	2023-01-01 10:08:00	150
4	2023-01-01 10:13:00	120
1	2023-01-01 10:05:00	100
5	2023-01-01 10:17:00	80
Time taken: 10.607 seconds
hive> select * from purchase sort by amount desc limit 1;
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0026, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0026
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0026
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 03:33:10,500 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:33:12,508 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.59 sec
2023-07-20 03:33:13,512 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.59 sec
2023-07-20 03:33:14,517 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.59 sec
2023-07-20 03:33:15,523 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.06 sec
2023-07-20 03:33:16,527 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.06 sec
2023-07-20 03:33:17,539 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.06 sec
MapReduce Total cumulative CPU time: 2 seconds 60 msec
Ended Job = job_202307191745_0026
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0027, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0027
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0027
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-07-20 03:33:20,974 Stage-2 map = 0%,  reduce = 0%
2023-07-20 03:33:22,983 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-07-20 03:33:23,987 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-07-20 03:33:24,991 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-07-20 03:33:25,995 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.23 sec
2023-07-20 03:33:27,000 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.23 sec
2023-07-20 03:33:28,014 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.23 sec
MapReduce Total cumulative CPU time: 2 seconds 230 msec
Ended Job = job_202307191745_0027
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.06 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 2.23 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 290 msec
OK
3	2023-01-01 10:09:00	200
Time taken: 20.988 seconds
hive> select * from customer cluster by userId;          
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0028, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0028
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0028
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 03:35:02,610 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:35:05,620 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.61 sec
2023-07-20 03:35:06,624 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.61 sec
2023-07-20 03:35:07,631 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.76 sec
2023-07-20 03:35:08,637 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.76 sec
2023-07-20 03:35:09,681 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.76 sec
MapReduce Total cumulative CPU time: 1 seconds 760 msec
Ended Job = job_202307191745_0028
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 1.76 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 760 msec
OK
1	John Doe	john.doe@example.com
2	Jane Smith	jane.smith@example.com
3	Robert Johnson	robert.johnson@example.com
4	Lisa Brown	lisa.brown@example.com
5	Michael Wilson	michael.wilson@example.com
Time taken: 10.948 seconds
hive> select * from customer join purchase;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0029, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0029
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0029
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 03:36:31,110 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:36:35,127 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.21 sec
2023-07-20 03:36:36,132 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.21 sec
2023-07-20 03:36:37,136 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.21 sec
2023-07-20 03:36:38,142 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.93 sec
2023-07-20 03:36:39,146 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.93 sec
2023-07-20 03:36:40,156 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.93 sec
MapReduce Total cumulative CPU time: 3 seconds 930 msec
Ended Job = job_202307191745_0029
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 3.93 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 930 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:05:00	100
1	John Doe	john.doe@example.com	2	2023-01-01 10:08:00	150
1	John Doe	john.doe@example.com	3	2023-01-01 10:09:00	200
1	John Doe	john.doe@example.com	4	2023-01-01 10:13:00	120
1	John Doe	john.doe@example.com	5	2023-01-01 10:17:00	80
2	Jane Smith	jane.smith@example.com	1	2023-01-01 10:05:00	100
2	Jane Smith	jane.smith@example.com	2	2023-01-01 10:08:00	150
2	Jane Smith	jane.smith@example.com	3	2023-01-01 10:09:00	200
2	Jane Smith	jane.smith@example.com	4	2023-01-01 10:13:00	120
2	Jane Smith	jane.smith@example.com	5	2023-01-01 10:17:00	80
3	Robert Johnson	robert.johnson@example.com	1	2023-01-01 10:05:00	100
3	Robert Johnson	robert.johnson@example.com	2	2023-01-01 10:08:00	150
3	Robert Johnson	robert.johnson@example.com	3	2023-01-01 10:09:00	200
3	Robert Johnson	robert.johnson@example.com	4	2023-01-01 10:13:00	120
3	Robert Johnson	robert.johnson@example.com	5	2023-01-01 10:17:00	80
4	Lisa Brown	lisa.brown@example.com	1	2023-01-01 10:05:00	100
4	Lisa Brown	lisa.brown@example.com	2	2023-01-01 10:08:00	150
4	Lisa Brown	lisa.brown@example.com	3	2023-01-01 10:09:00	200
4	Lisa Brown	lisa.brown@example.com	4	2023-01-01 10:13:00	120
4	Lisa Brown	lisa.brown@example.com	5	2023-01-01 10:17:00	80
5	Michael Wilson	michael.wilson@example.com	1	2023-01-01 10:05:00	100
5	Michael Wilson	michael.wilson@example.com	2	2023-01-01 10:08:00	150
5	Michael Wilson	michael.wilson@example.com	3	2023-01-01 10:09:00	200
5	Michael Wilson	michael.wilson@example.com	4	2023-01-01 10:13:00	120
5	Michael Wilson	michael.wilson@example.com	5	2023-01-01 10:17:00	80
Time taken: 12.611 seconds
hive> select * from customer full join purchase on userId=customer.userId;
FAILED: ParseException line 1:28 mismatched input 'join' expecting OUTER near 'full' in join type specifier

hive> select * from customer join purchase on userId=customer.userId;     
FAILED: SemanticException [Error 10009]: Line 1:40 Invalid table alias 'userId'
hive> select * from customer join purchase on purchase.userId=customer.userId;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0030, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0030
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0030
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 03:42:15,273 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:42:19,289 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.71 sec
2023-07-20 03:42:20,294 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.71 sec
2023-07-20 03:42:21,303 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.71 sec
2023-07-20 03:42:22,310 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.38 sec
2023-07-20 03:42:23,316 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.38 sec
2023-07-20 03:42:24,327 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.38 sec
MapReduce Total cumulative CPU time: 3 seconds 380 msec
Ended Job = job_202307191745_0030
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 3.38 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 380 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:05:00	100
2	Jane Smith	jane.smith@example.com	2	2023-01-01 10:08:00	150
3	Robert Johnson	robert.johnson@example.com	3	2023-01-01 10:09:00	200
4	Lisa Brown	lisa.brown@example.com	4	2023-01-01 10:13:00	120
5	Michael Wilson	michael.wilson@example.com	5	2023-01-01 10:17:00	80
Time taken: 12.623 seconds
hive> select * from customer join purchase on purchase.userId=customer.userId limit 2;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307191745_0031, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307191745_0031
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307191745_0031
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 03:46:27,890 Stage-1 map = 0%,  reduce = 0%
2023-07-20 03:46:31,904 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.95 sec
2023-07-20 03:46:32,908 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.95 sec
2023-07-20 03:46:33,912 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.95 sec
2023-07-20 03:46:34,918 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.7 sec
2023-07-20 03:46:35,921 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.7 sec
2023-07-20 03:46:36,931 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.7 sec
MapReduce Total cumulative CPU time: 3 seconds 700 msec
Ended Job = job_202307191745_0031
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 3.7 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 700 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:05:00	100
2	Jane Smith	jane.smith@example.com	2	2023-01-01 10:08:00	150
Time taken: 12.59 seconds
hive> 
